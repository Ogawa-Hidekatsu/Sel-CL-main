Namespace(DA='complex', alpha=0.5, alpha_m=1.0, alpha_moving=0.999, aprox=1, batch_size=128, beta=0.25, cuda_dev=0, data_path='./data/', dataset='CIFAR-10', device='cpu', download=True, epoch=250, epochs=100, experiment_name='Proof', headType='Linear', initial_epoch=1, k_val=250, lambda_c=1, lambda_s=0.01, load_local=False, loss='mse', low_dim=128, lr=0.1, lr_decay_epochs=[125, 200], lr_decay_rate=0.1, lr_scheduler='step', lr_warmup_epoch=1, lr_warmup_multiplier=100, momentum=0.9, nb_classes=10, network='PR18', noise_ratio=0.2, noise_type='symmetric', num_classes=10, num_workers=8, optimizer='sgd', out='./out', poisoning_rate=0.1, queue_per_class=1000, seed_dataset=42, seed_initialization=1, sup_queue_begin=3, sup_queue_use=1, sup_t=0.1, test_batch_size=100, train_root='./dataset', trigger_label=1, trigger_path='../badnets/triggers/trigger_white.png', trigger_size=5, uns_queue_k=10000, uns_t=0.1, warmup_epoch=1, warmup_way='uns', wd=0.0001)
Files already downloaded and verified
Files already downloaded and verified
Namespace(DA='complex',
alpha=0.5,
alpha_m=1.0,
alpha_moving=0.999,
aprox=1,
batch_size=128,
best_acc=0,
beta=0.25,
cuda_dev=0,
data_path='./data/',
dataset='CIFAR-10',
device='cpu',
download=True,
epoch=250,
epochs=100,
experiment_name='Proof',
headType='Linear',
initial_epoch=1,
k_val=250,
lambda_c=1,
lambda_s=0.01,
load_local=False,
loss='mse',
low_dim=128,
lr=0.1,
lr_decay_epochs=[125,
200],
lr_decay_rate=0.1,
lr_scheduler='step',
lr_warmup_epoch=1,
lr_warmup_multiplier=100,
momentum=0.9,
nb_classes=10,
network='PR18',
noise_ratio=0.2,
noise_type='symmetric',
num_classes=10,
num_workers=8,
optimizer='sgd',
out='./out',
poisoning_rate=0.1,
queue_per_class=1000,
seed_dataset=42,
seed_initialization=1,
sup_queue_begin=3,
sup_queue_use=1,
sup_t=0.1,
test_batch_size=100,
train_root='./dataset',
trigger_label=1,
trigger_path='../badnets/triggers/trigger_white.png',
trigger_size=5,
uns_queue_k=10000,
uns_t=0.1,
warmup_epoch=1,
warmup_way='uns',
wd=0.0001)

# load dataset: CIFAR-10 
Transform =  Compose(
    ToTensor()
    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
)
Files already downloaded and verified
Poison 5000 over 50000 samples ( poisoning rate 0.1)
Number of the class = 10
Dataset CIFAR10Poison
    Number of datapoints: 50000
    Root location: ./data/
    Split: Train
    StandardTransform
Transform: <other_utils.TwoCropTransform object at 0x7f20ac19eb20>
Target transform: Compose(
                      ToTensor()
                      Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
                  )
Transform =  Compose(
    ToTensor()
    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
)
Files already downloaded and verified
Files already downloaded and verified
Poison 10000 over 10000 samples ( poisoning rate 1.0)
Number of the class = 10
Dataset CIFAR10
    Number of datapoints: 10000
    Root location: ./data/
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
           ) Dataset CIFAR10Poison
    Number of datapoints: 10000
    Root location: ./data/
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
           )
############# Data loaded #############
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError("No module named 'amp_C'")
Namespace(DA='complex', alpha=0.5, alpha_m=1.0, alpha_moving=0.999, aprox=1, batch_size=128, beta=0.25, cuda_dev=0, data_path='./data/', dataset='CIFAR-10', device='cpu', download=True, epoch=250, epochs=100, experiment_name='Proof', headType='Linear', initial_epoch=1, k_val=250, lambda_c=1, lambda_s=0.01, load_local=False, loss='mse', low_dim=128, lr=0.1, lr_decay_epochs=[125, 200], lr_decay_rate=0.1, lr_scheduler='step', lr_warmup_epoch=1, lr_warmup_multiplier=100, momentum=0.9, nb_classes=10, network='PR18', noise_ratio=0.2, noise_type='symmetric', num_classes=10, num_workers=8, optimizer='sgd', out='./out', poisoning_rate=0.1, queue_per_class=1000, seed_dataset=42, seed_initialization=1, sup_queue_begin=3, sup_queue_use=1, sup_t=0.1, test_batch_size=100, train_root='./dataset', trigger_label=1, trigger_path='../badnets/triggers/trigger_white.png', trigger_size=5, uns_queue_k=10000, uns_t=0.1, warmup_epoch=1, warmup_way='uns', wd=0.0001)
Files already downloaded and verified
clean_num 40962
Files already downloaded and verified
Namespace(DA='complex',
alpha=0.5,
alpha_m=1.0,
alpha_moving=0.999,
aprox=1,
batch_size=128,
best_acc=0,
beta=0.25,
cuda_dev=0,
data_path='./data/',
dataset='CIFAR-10',
device='cpu',
download=True,
epoch=250,
epochs=100,
experiment_name='Proof',
headType='Linear',
initial_epoch=1,
k_val=250,
lambda_c=1,
lambda_s=0.01,
load_local=False,
loss='mse',
low_dim=128,
lr=0.1,
lr_decay_epochs=[125,
200],
lr_decay_rate=0.1,
lr_scheduler='step',
lr_warmup_epoch=1,
lr_warmup_multiplier=100,
momentum=0.9,
nb_classes=10,
network='PR18',
noise_ratio=0.2,
noise_type='symmetric',
num_classes=10,
num_workers=8,
optimizer='sgd',
out='./out',
poisoning_rate=0.1,
queue_per_class=1000,
seed_dataset=42,
seed_initialization=1,
sup_queue_begin=3,
sup_queue_use=1,
sup_t=0.1,
test_batch_size=100,
train_root='./dataset',
trigger_label=1,
trigger_path='../badnets/triggers/trigger_white.png',
trigger_size=5,
uns_queue_k=10000,
uns_t=0.1,
warmup_epoch=1,
warmup_way='uns',
wd=0.0001)

# load dataset: CIFAR-10 
Transform =  Compose(
    ToTensor()
    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
)
Files already downloaded and verified
Poison 5000 over 50000 samples ( poisoning rate 0.1)
Number of the class = 10
Dataset CIFAR10Poison
    Number of datapoints: 50000
    Root location: ./data/
    Split: Train
    StandardTransform
Transform: <other_utils.TwoCropTransform object at 0x7f7bce9daca0>
Target transform: Compose(
                      ToTensor()
                      Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
                  )
Transform =  Compose(
    ToTensor()
    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
)
Files already downloaded and verified
Files already downloaded and verified
Poison 10000 over 10000 samples ( poisoning rate 1.0)
Number of the class = 10
Dataset CIFAR10
    Number of datapoints: 10000
    Root location: ./data/
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
           ) Dataset CIFAR10Poison
    Number of datapoints: 10000
    Root location: ./data/
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
           )
############# Data loaded #############
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError("No module named 'amp_C'")
Namespace(DA='complex', alpha=0.5, alpha_m=1.0, alpha_moving=0.999, aprox=1, batch_size=128, beta=0.25, cuda_dev=0, data_path='./data/', dataset='CIFAR-10', device='cpu', download=True, epoch=250, epochs=100, experiment_name='Proof', headType='Linear', initial_epoch=1, k_val=250, lambda_c=1, lambda_s=0.01, load_local=False, loss='mse', low_dim=128, lr=0.1, lr_decay_epochs=[125, 200], lr_decay_rate=0.1, lr_scheduler='step', lr_warmup_epoch=1, lr_warmup_multiplier=100, momentum=0.9, nb_classes=10, network='PR18', noise_ratio=0.2, noise_type='symmetric', num_classes=10, num_workers=8, optimizer='sgd', out='./out', poisoning_rate=0.1, queue_per_class=1000, seed_dataset=42, seed_initialization=1, sup_queue_begin=3, sup_queue_use=1, sup_t=0.1, test_batch_size=100, train_root='./dataset', trigger_label=1, trigger_path='../badnets/triggers/trigger_white.png', trigger_size=5, uns_queue_k=10000, uns_t=0.1, warmup_epoch=1, warmup_way='uns', wd=0.0001)
Namespace(DA='complex',
alpha=0.5,
alpha_m=1.0,
alpha_moving=0.999,
aprox=1,
batch_size=128,
best_acc=0,
beta=0.25,
cuda_dev=0,
data_path='./data/',
dataset='CIFAR-10',
device='cpu',
download=True,
epoch=250,
epochs=100,
experiment_name='Proof',
headType='Linear',
initial_epoch=1,
k_val=250,
lambda_c=1,
lambda_s=0.01,
load_local=False,
loss='mse',
low_dim=128,
lr=0.1,
lr_decay_epochs=[125,
200],
lr_decay_rate=0.1,
lr_scheduler='step',
lr_warmup_epoch=1,
lr_warmup_multiplier=100,
momentum=0.9,
nb_classes=10,
network='PR18',
noise_ratio=0.2,
noise_type='symmetric',
num_classes=10,
num_workers=8,
optimizer='sgd',
out='./out',
poisoning_rate=0.1,
queue_per_class=1000,
seed_dataset=42,
seed_initialization=1,
sup_queue_begin=3,
sup_queue_use=1,
sup_t=0.1,
test_batch_size=100,
train_root='./dataset',
trigger_label=1,
trigger_path='../badnets/triggers/trigger_white.png',
trigger_size=5,
uns_queue_k=10000,
uns_t=0.1,
warmup_epoch=1,
warmup_way='uns',
wd=0.0001)

# load dataset: CIFAR-10 
Transform =  Compose(
    ToTensor()
    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
)
Files already downloaded and verified
Poison 5000 over 50000 samples ( poisoning rate 0.1)
Number of the class = 10
Dataset CIFAR10Poison
    Number of datapoints: 50000
    Root location: ./data/
    Split: Train
    StandardTransform
Transform: <other_utils.TwoCropTransform object at 0x7f8f731a0790>
Target transform: Compose(
                      ToTensor()
                      Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
                  )
Transform =  Compose(
    ToTensor()
    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
)
Files already downloaded and verified
Files already downloaded and verified
Poison 10000 over 10000 samples ( poisoning rate 1.0)
Number of the class = 10
Dataset CIFAR10
    Number of datapoints: 10000
    Root location: ./data/
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
           ) Dataset CIFAR10Poison
    Number of datapoints: 10000
    Root location: ./data/
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
           )
############# Data loaded #############
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError("No module named 'amp_C'")
Namespace(DA='complex', alpha=0.5, alpha_m=1.0, alpha_moving=0.999, aprox=1, batch_size=128, beta=0.25, cuda_dev=0, data_path='./data/', dataset='CIFAR-10', device='cpu', download=True, epoch=250, epochs=100, experiment_name='Proof', headType='Linear', initial_epoch=1, k_val=250, lambda_c=1, lambda_s=0.01, load_local=False, loss='mse', low_dim=128, lr=0.1, lr_decay_epochs=[125, 200], lr_decay_rate=0.1, lr_scheduler='step', lr_warmup_epoch=1, lr_warmup_multiplier=100, momentum=0.9, nb_classes=10, network='PR18', noise_ratio=0.2, noise_type='symmetric', num_classes=10, num_workers=8, optimizer='sgd', out='./out', poisoning_rate=0.1, queue_per_class=1000, seed_dataset=42, seed_initialization=1, sup_queue_begin=3, sup_queue_use=1, sup_t=0.1, test_batch_size=100, train_root='./dataset', trigger_label=1, trigger_path='../badnets/triggers/trigger_white.png', trigger_size=5, uns_queue_k=10000, uns_t=0.1, warmup_epoch=1, warmup_way='uns', wd=0.0001)
Namespace(DA='complex',
alpha=0.5,
alpha_m=1.0,
alpha_moving=0.999,
aprox=1,
batch_size=128,
best_acc=0,
beta=0.25,
cuda_dev=0,
data_path='./data/',
dataset='CIFAR-10',
device='cpu',
download=True,
epoch=250,
epochs=100,
experiment_name='Proof',
headType='Linear',
initial_epoch=1,
k_val=250,
lambda_c=1,
lambda_s=0.01,
load_local=False,
loss='mse',
low_dim=128,
lr=0.1,
lr_decay_epochs=[125,
200],
lr_decay_rate=0.1,
lr_scheduler='step',
lr_warmup_epoch=1,
lr_warmup_multiplier=100,
momentum=0.9,
nb_classes=10,
network='PR18',
noise_ratio=0.2,
noise_type='symmetric',
num_classes=10,
num_workers=8,
optimizer='sgd',
out='./out',
poisoning_rate=0.1,
queue_per_class=1000,
seed_dataset=42,
seed_initialization=1,
sup_queue_begin=3,
sup_queue_use=1,
sup_t=0.1,
test_batch_size=100,
train_root='./dataset',
trigger_label=1,
trigger_path='../badnets/triggers/trigger_white.png',
trigger_size=5,
uns_queue_k=10000,
uns_t=0.1,
warmup_epoch=1,
warmup_way='uns',
wd=0.0001)

# load dataset: CIFAR-10 
Transform =  Compose(
    ToTensor()
    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
)
Files already downloaded and verified
Poison 5000 over 50000 samples ( poisoning rate 0.1)
Number of the class = 10
Dataset CIFAR10Poison
    Number of datapoints: 50000
    Root location: ./data/
    Split: Train
    StandardTransform
Transform: <other_utils.TwoCropTransform object at 0x7fa5b8061a90>
Target transform: Compose(
                      ToTensor()
                      Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
                  )
Transform =  Compose(
    ToTensor()
    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
)
Files already downloaded and verified
Files already downloaded and verified
Poison 10000 over 10000 samples ( poisoning rate 1.0)
Number of the class = 10
Dataset CIFAR10
    Number of datapoints: 10000
    Root location: ./data/
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
           ) Dataset CIFAR10Poison
    Number of datapoints: 10000
    Root location: ./data/
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
           )
############# Data loaded #############
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError("No module named 'amp_C'")
=================>     Proof 0.2
Train Epoch: 1 [1920/50000 (4%)]	Loss: 0.000000, Learning rate: 0.004798
Train Epoch: 1 [3840/50000 (8%)]	Loss: 0.000000, Learning rate: 0.008596
Train Epoch: 1 [5760/50000 (12%)]	Loss: 0.000000, Learning rate: 0.012394
Train Epoch: 1 [7680/50000 (15%)]	Loss: 0.000000, Learning rate: 0.016192
Train Epoch: 1 [9600/50000 (19%)]	Loss: 0.000000, Learning rate: 0.019990
Train Epoch: 1 [11520/50000 (23%)]	Loss: 0.000000, Learning rate: 0.023788
Train Epoch: 1 [13440/50000 (27%)]	Loss: 0.000000, Learning rate: 0.027586
Train Epoch: 1 [15360/50000 (31%)]	Loss: 0.000000, Learning rate: 0.031384
Train Epoch: 1 [17280/50000 (35%)]	Loss: 0.000000, Learning rate: 0.035182
Train Epoch: 1 [19200/50000 (38%)]	Loss: 0.000000, Learning rate: 0.038980
Train Epoch: 1 [21120/50000 (42%)]	Loss: 0.000000, Learning rate: 0.042777
Train Epoch: 1 [23040/50000 (46%)]	Loss: 0.000000, Learning rate: 0.046575
Train Epoch: 1 [24960/50000 (50%)]	Loss: 0.000000, Learning rate: 0.050373
Train Epoch: 1 [26880/50000 (54%)]	Loss: 0.000000, Learning rate: 0.054171
Train Epoch: 1 [28800/50000 (58%)]	Loss: 0.000000, Learning rate: 0.057969
Train Epoch: 1 [30720/50000 (61%)]	Loss: 0.000000, Learning rate: 0.061767
Train Epoch: 1 [32640/50000 (65%)]	Loss: 0.000000, Learning rate: 0.065565
Train Epoch: 1 [34560/50000 (69%)]	Loss: 0.000000, Learning rate: 0.069363
Train Epoch: 1 [36480/50000 (73%)]	Loss: 0.000000, Learning rate: 0.073161
Train Epoch: 1 [38400/50000 (77%)]	Loss: 0.000000, Learning rate: 0.076959
Train Epoch: 1 [40320/50000 (81%)]	Loss: 0.000000, Learning rate: 0.080757
Train Epoch: 1 [42240/50000 (84%)]	Loss: 0.000000, Learning rate: 0.084555
Train Epoch: 1 [44160/50000 (88%)]	Loss: 0.000000, Learning rate: 0.088353
Train Epoch: 1 [46080/50000 (92%)]	Loss: 0.000000, Learning rate: 0.092151
Train Epoch: 1 [48000/50000 (96%)]	Loss: 0.000000, Learning rate: 0.095949
Train Epoch: 1 [49920/50000 (100%)]	Loss: 0.000000, Learning rate: 0.099747
train_uns_loss 7.766807270660401
train time 25.68888568878174
######## Pair-wise selection ########
